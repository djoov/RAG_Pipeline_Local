RAG Pipeline Lokal dengan Ollama (Modifikasi dari Proyek Asli pixegami)Repositori ini adalah versi yang telah dimodifikasi dan disederhanakan secara signifikan dari proyek simple-rag-pipeline oleh pixegami. Modifikasi ini bertujuan untuk membuat sebuah sistem RAG (Retrieval-Augmented Generation) yang berjalan 100% lokal menggunakan Ollama, dengan fokus pada kemudahan penggunaan, stabilitas, dan fleksibilitas.Seluruh proses, mulai dari memproses dokumen hingga menghasilkan jawaban, terjadi sepenuhnya di komputer Anda, memastikan privasi data terjaga dan tidak memerlukan koneksi internet setelah penyiapan awal.Diagram oleh pixegami‚ú® Fitur Utama & ModifikasiProyek ini mengambil konsep inti dari repositori asli dan memperbaikinya dengan beberapa modifikasi kunci:100% Lokal & Offline: Menggunakan Ollama untuk menjalankan model embedding dan LLM secara lokal. Tidak ada data yang dikirim ke API eksternal.Pemrosesan Dokumen yang Andal: Mengganti metode pemecahan teks asli dengan LangChain (PyPDFLoader dan RecursiveCharacterTextSplitter) untuk memastikan dokumen PDF dibaca dengan benar dan dipecah menjadi potongan (chunks) dengan ukuran yang konsisten, sehingga menghilangkan error fundamental.Eksekusi Skrip Tunggal yang Disederhanakan: Struktur proyek yang kompleks telah disatukan menjadi satu skrip utama, run_local.py, yang menangani seluruh alur kerja dari awal hingga akhir.Mode Chat Interaktif: Setelah dokumen diproses, Anda bisa langsung bertanya jawab dengan dokumen Anda secara terus-menerus melalui terminal.Dukungan Model yang Fleksibel: Sangat mudah untuk mengganti model embedding atau model LLM penjawab hanya dengan mengubah nama model di dalam skrip, selama model tersebut tersedia di Ollama.‚öôÔ∏è Cara KerjaAlur kerja sistem RAG ini sederhana namun kuat:Load & Chunk: Skrip akan membaca semua file PDF yang Anda letakkan di folder sample_data/source/. Menggunakan LangChain, setiap dokumen dipecah menjadi potongan-potongan teks yang lebih kecil dan mudah dikelola.Embedding: Setiap potongan teks diubah menjadi representasi numerik (vektor embedding) menggunakan model embedding yang berjalan di Ollama (contoh: nomic-embed-text). Proses ini seperti membuat "indeks makna" untuk seluruh dokumen Anda.Retrieval: Saat Anda mengajukan pertanyaan, pertanyaan tersebut juga diubah menjadi vektor embedding. Sistem kemudian mencari dan mengambil beberapa potongan teks dari dokumen yang vektornya paling mirip secara matematis dengan vektor pertanyaan Anda.Generation: Potongan-potongan teks yang paling relevan tersebut (disebut sebagai "konteks") digabungkan dengan pertanyaan asli Anda. Keduanya kemudian diberikan kepada Large Language Model (LLM) penjawab (contoh: phi3:mini atau llama3:8b) yang juga berjalan di Ollama. LLM kemudian menghasilkan jawaban dalam bahasa alami berdasarkan konteks yang diberikan.üöÄ PrasyaratSebelum memulai, pastikan Anda telah menginstal:Python 3.10+Ollama: Unduh dan instal dari situs web resmi Ollama. Pastikan layanan Ollama berjalan di latar belakang.üõ†Ô∏è Penyiapan & InstalasiIkuti langkah-langkah berikut untuk menyiapkan proyek:Clone Repositorigit clone [URL_REPOSITORI_ANDA]
cd [NAMA_FOLDER_REPOSITORI]
Buat Lingkungan Virtual (Virtual Environment)Sangat disarankan untuk menggunakan lingkungan virtual agar dependensi proyek tidak tercampur dengan instalasi Python global Anda.# Windows
python -m venv venv
.\venv\Scripts\activate

# macOS / Linux
python3 -m venv venv
source venv/bin/activate
Instal DependensiInstal semua library Python yang dibutuhkan dengan satu perintah:pip install -r requirements.txt
Unduh Model OllamaBuka terminal Anda dan unduh model yang diperlukan. Anda memerlukan satu model embedding dan satu model LLM.Model Embedding (Wajib):ollama pull nomic-embed-text
Model LLM Penjawab (Pilih salah satu, phi3:mini direkomendasikan untuk awal):# Pilihan terbaik untuk keseimbangan performa & kualitas di VRAM < 6GB
ollama pull phi3:mini

# Pilihan lebih kuat jika VRAM Anda mencukupi (>6GB)
ollama pull llama3:8b
‚ñ∂Ô∏è Cara MenjalankanTambahkan Dokumen AndaLetakkan semua file PDF yang ingin Anda jadikan basis pengetahuan ke dalam folder sample_data/source/. Anda bisa menghapus file PDF contoh yang ada.Jalankan Skrip UtamaPastikan Anda berada di direktori utama proyek dan lingkungan virtual (venv) Anda aktif. Kemudian, jalankan perintah:python run_local.py
Proses & Tanya JawabProgram akan mulai bekerja:Pertama, ia akan memproses semua PDF di folder sumber.Kedua, ia akan membuat embedding untuk setiap potongan teks. Proses ini mungkin memakan waktu beberapa saat tergantung pada jumlah dokumen dan kecepatan komputer Anda.Setelah selesai, Anda akan melihat prompt: Masukkan pertanyaan Anda (atau ketik 'keluar' untuk berhenti):Ketik pertanyaan Anda, tekan Enter, dan tunggu model menghasilkan jawaban.üîß Kustomisasi & EksperimenAnda dapat dengan mudah menyesuaikan pipeline ini untuk mendapatkan hasil yang lebih baik:Mengganti Model LLMBuka file run_local.py dan ubah nilai variabel LLM_MODEL. Pastikan model yang Anda pilih sudah diunduh melalui Ollama.# Ganti dari:
LLM_MODEL = "gemma:2b"

# Menjadi (contoh):
LLM_MODEL = "phi3:mini"
Mengganti Model EmbeddingJika diperlukan, Anda juga bisa mengganti model embedding. Model alternatif yang bagus adalah mxbai-embed-large.# Unduh model baru
# ollama pull mxbai-embed-large

# Ganti di skrip:
EMBEDDING_MODEL = "mxbai-embed-large"
Penting: Jika Anda mengganti EMBEDDING_MODEL, seluruh proses pengindeksan akan berjalan kembali saat Anda menjalankan skrip.Menyesuaikan Ukuran Potongan Teks (Chunking)Untuk dokumen dengan struktur yang berbeda, Anda mungkin perlu menyesuaikan cara teks dipecah. Ubah parameter chunk_size dan chunk_overlap di dalam fungsi load_and_chunk_documents.# Eksperimen dengan nilai yang berbeda
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)
üôè Ucapan Terima KasihProyek ini tidak akan ada tanpa kerja keras dan kode sumber terbuka dari pixegami. Repositori ini hanyalah sebuah modifikasi untuk tujuan pembelajaran dan penggunaan lokal yang lebih sederhana.Silakan kunjungi repositori aslinya di: https://github.com/pixegami/simple-rag-pipeline
